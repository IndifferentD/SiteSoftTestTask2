
# Программа проверки правил для документа

Реализовать систему, которая для документа сможет проверить, подходит ли он под правила вида:
“слово1 и (слово2 или слово3) и не слово4”, например: “центр и (Екатринбург или Екб) и не Москва”.
---
Логическое выражение задается в файле `input_expression.txt`
Текст для анализа - в файле `input_text.txt`


Запуск:
`pip install --no-cache-dir -r requirements.txt`
`python main.py`

Усложнения:

✅ Реализовать проверку слов так, чтобы морфология слова не влияла на результат проверки

✅ Оценить алгоритмическую сложность решения

    main()

| Этап                                                                                                                         | time                           | space        | примечание                                                                                                                                                                                 |
|------------------------------------------------------------------------------------------------------------------------------|--------------------------------|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| чтение файлов `input_text.txt` и `input_expression.txt`                                                                              | `O(N)` и `O(M)` соответственно | `O(1) `      |                                                                                                                                                                                            |
| `remove_parentheses_and_punctuation ()`                                                                                      | `O(2N)`                        | `O(2N)`      | дважды используется `re.sub`                                                                                                                                                               |
| `space_the_parentheses()`                                                                                                    | `O(M)`                         | `O(M)`       ||
| `get_postfix_notation_and_words_to_search()`                                                                                 | `O(M)`                         | `O(3M)`      | `3M` по памяти т.к. создаем словарь с нормальными формами слов для поиска, массив под результат и массив под операторы  |
| создание сета `words_to_search_forms`                                                                                        | `O(Mk)=O(M)`                   | `O(Mk)=O(M)` | где k - максимально возможное количество форм для слова                                                                                                                                    |
| проход по тексту `for word in input_text` и выставление метки о наличии слова в тексте `words_to_search[normal_form] = True` | `O(N)`                         | `O(1)`       | т.к. у нас есть сет `words_to_search_forms` то операция проверки наличия слова в тексте занимает `O(1)`                                                                                    |
| `evaluate_postfix_notation()`                                                                                                | `O(M)`                         | `O(M)`       | линейных проход по массиву с токенами `postfix_representation` и накоплением стека операндов для выполнения ближайшей операции                                                             |
Итого имеем линейную сложность алгоритма по памяти и времени

- Как можно оптимизировать систему для проверки сотен подобных правил для каждого документа?


1. В данной реализации для оптимизации времени выполнения (но увеличения затрат памяти) можно создавать общий словарь `words_to_search` и сет `words_to_search_forms`, в которых будут учтены слова для поиска из всех "запросов", после чего можно выполнить `evaluate_postfix_notation()` для каждого запроса, не анализируя текст заново
2. При других реализациях возможно будет уместно сокращать логические выражения перед их вычислением, а также кэшировать результаты бинарных операций, если операции производится непосредственно над словами

